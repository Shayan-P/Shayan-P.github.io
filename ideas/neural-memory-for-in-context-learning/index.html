<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Dynamic Neural Memory for In-Context Learning: SSMs or Transformers? | Shayan Pardis</title>
<meta name=keywords content="NLP,Model Architecture"><meta name=description content="Motivation
This idea was inspired by a talk that Albert Gu gave at Simons Institute Workshop Transformers as a Computational Model (link to the talk).
While listening to this talk, I began to think about how we might design a new memory model by combining features from both State Space Models and transformers.
State Space Models 




(

          Citation: 
              Gu
               et al., 2021 










Gu, 
    
    A., 
  Goel, 
    
    K. & R√©, 
    
    C.
   
    (2021).
   Efficiently modeling long sequences with structured state spaces.
    CoRR, abs/2111.00396. Retrieved from 
  https://arxiv.org/abs/2111.00396




)
 compress all the history in a finite memory.
By contrast, transformers 




(

          Citation: 
              Vaswani
               et al., 2017 










Vaswani, 
    
    A., 
  Shazeer, 
    
    N., 
  Parmar, 
    
    N., 
  Uszkoreit, 
    
    J., 
  Jones, 
    
    L., 
  Gomez, 
    
    A., 
  Kaiser, 
    
    L. & Polosukhin, 
    
    I.
   
    (2017).
   Attention is all you need.
    CoRR, abs/1706.03762. Retrieved from 
  http://arxiv.org/abs/1706.03762




)
 keep all the context. Intuitively, transformers function like a database with $O(n)$ memory growth, while SSMs focus on compressing the most critical information from the context into $O(1)$ memory."><meta name=author content="Shayan Pardis"><link rel=canonical href=https://shayan-p.github.io/ideas/neural-memory-for-in-context-learning/><link crossorigin=anonymous href=/assets/css/stylesheet.a8917d93858164ff031315539f0df2b9f5eff75874d4258dc6215bc53778b0b3.css integrity="sha256-qJF9k4WBZP8DExVTnw3yufXv91h01CWNxiFbxTd4sLM=" rel="preload stylesheet" as=style><link rel=icon href=https://shayan-p.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://shayan-p.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://shayan-p.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://shayan-p.github.io/apple-touch-icon.png><link rel=mask-icon href=https://shayan-p.github.io/safari-pinned-tab.svg><link rel=stylesheet href=/css/custom.css><script src=/js/custom.js defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://shayan-p.github.io/ideas/neural-memory-for-in-context-learning/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-NB76RCJ039"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NB76RCJ039")}</script><meta property="og:title" content="Dynamic Neural Memory for In-Context Learning: SSMs or Transformers?"><meta property="og:description" content="Motivation
This idea was inspired by a talk that Albert Gu gave at Simons Institute Workshop Transformers as a Computational Model (link to the talk).
While listening to this talk, I began to think about how we might design a new memory model by combining features from both State Space Models and transformers.
State Space Models 




(

          Citation: 
              Gu
               et al., 2021 










Gu, 
    
    A., 
  Goel, 
    
    K. & R√©, 
    
    C.
   
    (2021).
   Efficiently modeling long sequences with structured state spaces.
    CoRR, abs/2111.00396. Retrieved from 
  https://arxiv.org/abs/2111.00396




)
 compress all the history in a finite memory.
By contrast, transformers 




(

          Citation: 
              Vaswani
               et al., 2017 










Vaswani, 
    
    A., 
  Shazeer, 
    
    N., 
  Parmar, 
    
    N., 
  Uszkoreit, 
    
    J., 
  Jones, 
    
    L., 
  Gomez, 
    
    A., 
  Kaiser, 
    
    L. & Polosukhin, 
    
    I.
   
    (2017).
   Attention is all you need.
    CoRR, abs/1706.03762. Retrieved from 
  http://arxiv.org/abs/1706.03762




)
 keep all the context. Intuitively, transformers function like a database with $O(n)$ memory growth, while SSMs focus on compressing the most critical information from the context into $O(1)$ memory."><meta property="og:type" content="article"><meta property="og:url" content="https://shayan-p.github.io/ideas/neural-memory-for-in-context-learning/"><meta property="article:section" content="ideas"><meta property="article:published_time" content="2024-10-30T01:23:20-04:00"><meta property="article:modified_time" content="2024-10-30T01:23:20-04:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Dynamic Neural Memory for In-Context Learning: SSMs or Transformers?"><meta name=twitter:description content="Motivation
This idea was inspired by a talk that Albert Gu gave at Simons Institute Workshop Transformers as a Computational Model (link to the talk).
While listening to this talk, I began to think about how we might design a new memory model by combining features from both State Space Models and transformers.
State Space Models 




(

          Citation: 
              Gu
               et al., 2021 










Gu, 
    
    A., 
  Goel, 
    
    K. & R√©, 
    
    C.
   
    (2021).
   Efficiently modeling long sequences with structured state spaces.
    CoRR, abs/2111.00396. Retrieved from 
  https://arxiv.org/abs/2111.00396




)
 compress all the history in a finite memory.
By contrast, transformers 




(

          Citation: 
              Vaswani
               et al., 2017 










Vaswani, 
    
    A., 
  Shazeer, 
    
    N., 
  Parmar, 
    
    N., 
  Uszkoreit, 
    
    J., 
  Jones, 
    
    L., 
  Gomez, 
    
    A., 
  Kaiser, 
    
    L. & Polosukhin, 
    
    I.
   
    (2017).
   Attention is all you need.
    CoRR, abs/1706.03762. Retrieved from 
  http://arxiv.org/abs/1706.03762




)
 keep all the context. Intuitively, transformers function like a database with $O(n)$ memory growth, while SSMs focus on compressing the most critical information from the context into $O(1)$ memory."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Idea Book üìîüñäÔ∏è","item":"https://shayan-p.github.io/ideas/"},{"@type":"ListItem","position":2,"name":"Dynamic Neural Memory for In-Context Learning: SSMs or Transformers?","item":"https://shayan-p.github.io/ideas/neural-memory-for-in-context-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Dynamic Neural Memory for In-Context Learning: SSMs or Transformers?","name":"Dynamic Neural Memory for In-Context Learning: SSMs or Transformers?","description":"Motivation This idea was inspired by a talk that Albert Gu gave at Simons Institute Workshop Transformers as a Computational Model (link to the talk). While listening to this talk, I began to think about how we might design a new memory model by combining features from both State Space Models and transformers.\nState Space Models ( Citation: Gu \u0026#32;et al.,\u0026#32;2021 Gu,\u0026#32; A.,\u0026#32; Goel,\u0026#32; K.\u0026#32;\u0026amp;\u0026#32;R√©,\u0026#32; C. \u0026#32; (2021). \u0026#32;Efficiently modeling long sequences with structured state spaces. CoRR,\u0026#32;abs/2111.00396.\u0026#32;Retrieved from\u0026#32; https://arxiv.org/abs/2111.00396 ) compress all the history in a finite memory. By contrast, transformers ( Citation: Vaswani \u0026#32;et al.,\u0026#32;2017 Vaswani,\u0026#32; A.,\u0026#32; Shazeer,\u0026#32; N.,\u0026#32; Parmar,\u0026#32; N.,\u0026#32; Uszkoreit,\u0026#32; J.,\u0026#32; Jones,\u0026#32; L.,\u0026#32; Gomez,\u0026#32; A.,\u0026#32; Kaiser,\u0026#32; L.\u0026#32;\u0026amp;\u0026#32;Polosukhin,\u0026#32; I. \u0026#32; (2017). \u0026#32;Attention is all you need. CoRR,\u0026#32;abs/1706.03762.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1706.03762 ) keep all the context. Intuitively, transformers function like a database with $O(n)$ memory growth, while SSMs focus on compressing the most critical information from the context into $O(1)$ memory.\n","keywords":["NLP","Model Architecture"],"articleBody":"Motivation This idea was inspired by a talk that Albert Gu gave at Simons Institute Workshop Transformers as a Computational Model (link to the talk). While listening to this talk, I began to think about how we might design a new memory model by combining features from both State Space Models and transformers.\nState Space Models ( Citation: Gu et al., 2021 Gu, A., Goel, K. \u0026 R√©, C. (2021). Efficiently modeling long sequences with structured state spaces. CoRR, abs/2111.00396. Retrieved from https://arxiv.org/abs/2111.00396 ) compress all the history in a finite memory. By contrast, transformers ( Citation: Vaswani et al., 2017 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L. \u0026 Polosukhin, I. (2017). Attention is all you need. CoRR, abs/1706.03762. Retrieved from http://arxiv.org/abs/1706.03762 ) keep all the context. Intuitively, transformers function like a database with $O(n)$ memory growth, while SSMs focus on compressing the most critical information from the context into $O(1)$ memory.\nCan we use the best of both worlds and design a graph-like memory model that dynamically adds new nodes (like transformers, but with sublinear growth) while also learning to compress information hierarchically into rich latent representations (like SSMs)?\nRelated Work Human memory is associative; we remember concepts by their relationship to each other. Hopfield networks provide one of the earliest examples of associative memory in artificial neural networks. In Hopfield Network is All You Need ( Citation: Ramsauer et al., 2020 Ramsauer, H., Sch√§fl, B., Lehner, J., Seidl, P., Widrich, M., Gruber, L., Holzleitner, M., Pavlovic, M., Sandve, G., Greiff, V., Kreil, D., Kopp, M., Klambauer, G., Brandstetter, J. \u0026 Hochreiter, S. (2020). Hopfield networks is all you need. CoRR, abs/2008.02217. Retrieved from https://arxiv.org/abs/2008.02217 ) Hopfield layers are introduced and their convergence and capacity properties are analyzed.\nIn Memory Networks ( Citation: Weston et al., 2015 Weston, J., Chopra, S. \u0026 Bordes, A. (2015). Memory networks. Retrieved from https://arxiv.org/abs/1410.3916 ) a memory design with four components is introduced:\nfeature map $I$: this encodes the data in the latent space generalization component $G$: updates the memory with the new data output feature map $O$: produces output based on query response component $R$: this is essentially the decoder This is how the output feature map operates: $$O(x, m) = \\argmax_{i=1,‚Ä¶,N} s_O(x, m_i)$$ where $m_i$ are the memory elements, and $x$ is the featurized query, and $s_O$ is a score function. The score function they proposed was: $$ s_O(x, y) = \\Phi_x(x)^T U^T U \\Phi_y(y) $$\nInterestingly, this is conceptually similar to the attention mechanism in which we assign score $x^t \\frac{Q^T K}{d_k} y$ to a feature pair $x, y$. In this context, $U$ from above can be derived through Cholesky decomposition and the softmax function in attention replaces argmax in this formulation.\nDynamic Neural Memory Can we combine the best of both worlds: the compression mechanism of SSMs and the dynamic, database-like nature of transformers, to design a memory mechanism that grows sublinearly, perhaps $O(\\log n)$ with respect to input size? This would not only address transformers‚Äô context length limitation but also provide a balance between understanding/compressing and memorizing. Essentially, this is a tradeoff between memory and runtime search.\nIt‚Äôs very natural to think of memory as a graph of concepts where each concept has its own embedding.\nAdding new concepts involves traversing this graph, adding new nodes and edges, and enriching the latent representations of existing nodes. Retrieving information would involve a similar traversal, using the encoded query in the latent space and aggregating features from the visited nodes.\nA simple first step could be to design a neural binary search tree. In this design, each node has three learnable, parameterized functions: update, stop, and router. Adding new data could look like this:\ndef add_concept(memory_node, concept): update(memory_node, concept) if stop(memory_node, concept): return next_memory_node = router(memory_node, concept) add_concept(memory_node, concept) new_info = encode(information) add_concept(memory_node=memory.root, concept=new_info) In a graph-based structure, router could be implemented as an attention block. Rather than routing to a single node, it could calculate a probability distribution over adjacent nodes and pass the probability flow down the graph.\nPotential Issues Learnability: How do we define a learning mechanism that ensures convergence? In particular, designing an objective function for training may be complex due to the non-linear, high-dimensional updates involved in memory storage and retrieval. Scalability: The process described above is sequential. How can we limit the number of sequential steps required for memory updates, or find a balance between traversal depth (i.e., ‚Äúthinking time‚Äù) and accuracy? Final Thoughts The attention mechanism, at its core, functions as a read-memory operation. It became a breakthrough for deep learning due to its generality and scalability. However, attention-based memory operates more like a database and less like natural memory, which is hierarchically organized and capable of efficient retrieval based on relationships between concepts.\nAn improved memory design could offer hierarchical data storage, enhance generalization, improve data efficiency, and overcome the context length limitations in transformers. These advancements could significantly impact how we approach sequential problem-solving in NLP and beyond.\nOne motivating implication for me is in-context learning. Think of the memory model in world models ( Citation: Ha \u0026 Schmidhuber, 2018 Ha, D. \u0026 Schmidhuber, J. (2018). World models. https://doi.org/10.5281/ZENODO.1207631 ) . As an agent interacts with its environment, it receives a continuous stream of data, which it uses to build a representation of the environment. The effectiveness of the memory model determines how well the agent learns from past experiences and assigns credit to rewards. Integrating memory models into reinforcement learning has been explored in works such as Reinforcement Learning as One Big Sequence Modeling Problem ( Citation: Janner et al., 2021 Janner, M., Li, Q. \u0026 Levine, S. (2021). Reinforcement learning as one big sequence modeling problem. CoRR, abs/2106.02039. Retrieved from https://arxiv.org/abs/2106.02039 ) and Decision Transformer: Reinforcement Learning via Sequence Modeling ( Citation: Chen et al., 2021 Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A. \u0026 Mordatch, I. (2021). Decision transformer: Reinforcement learning via sequence modeling. Retrieved from https://arxiv.org/abs/2106.01345 ) .\nCitation Cited as:\nShayan Pardis. (Oct 2024). Dynamic Neural Memory for In-Context Learning: SSMs or Transformers?. https://shayan-p.github.io/ideas/neural-memory-for-in-context-learning/.\nOr @article{ shayan-pardis2024dynamic-neural-memory-for-in-context-learning-ssms-or-transformers, title = \"Dynamic Neural Memory for In-Context Learning: SSMs or Transformers?\", author = \"Shayan Pardis\", year = \"2024\", month = \"Oct\", url = \"https://shayan-p.github.io/ideas/neural-memory-for-in-context-learning/\" } copy References [1] Vaswani et al. Attention is all you need CoRR (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L. \u0026 Polosukhin, I. (2017). Attention is all you need. CoRR, abs/1706.03762. Retrieved from http://arxiv.org/abs/1706.03762 [2] Ramsauer et al. Hopfield networks is all you need CoRR (2020) Ramsauer, H., Sch√§fl, B., Lehner, J., Seidl, P., Widrich, M., Gruber, L., Holzleitner, M., Pavlovic, M., Sandve, G., Greiff, V., Kreil, D., Kopp, M., Klambauer, G., Brandstetter, J. \u0026 Hochreiter, S. (2020). Hopfield networks is all you need. CoRR, abs/2008.02217. Retrieved from https://arxiv.org/abs/2008.02217 [3] Janner et al. Reinforcement learning as one big sequence modeling problem CoRR (2021) Janner, M., Li, Q. \u0026 Levine, S. (2021). Reinforcement learning as one big sequence modeling problem. CoRR, abs/2106.02039. Retrieved from https://arxiv.org/abs/2106.02039 [4] Gu et al. Efficiently modeling long sequences with structured state spaces CoRR (2021) Gu, A., Goel, K. \u0026 R√©, C. (2021). Efficiently modeling long sequences with structured state spaces. CoRR, abs/2111.00396. Retrieved from https://arxiv.org/abs/2111.00396 [5] Chen et al. Decision transformer: Reinforcement learning via sequence modeling (2021) Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A. \u0026 Mordatch, I. (2021). Decision transformer: Reinforcement learning via sequence modeling. Retrieved from https://arxiv.org/abs/2106.01345 [6] Ha \u0026 Schmidhuber World models (2018) Ha, D. \u0026 Schmidhuber, J. (2018). World models. https://doi.org/10.5281/ZENODO.1207631 [7] Weston et al. Memory networks (2015) Weston, J., Chopra, S. \u0026 Bordes, A. (2015). Memory networks. Retrieved from https://arxiv.org/abs/1410.3916 ","wordCount":"1262","inLanguage":"en","datePublished":"2024-10-30T01:23:20-04:00","dateModified":"2024-10-30T01:23:20-04:00","author":{"@type":"Person","name":"Shayan Pardis"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://shayan-p.github.io/ideas/neural-memory-for-in-context-learning/"},"publisher":{"@type":"Organization","name":"Shayan Pardis","logo":{"@type":"ImageObject","url":"https://shayan-p.github.io/favicon.ico"}}}</script><link rel=stylesheet type=text/css href=/hugo-cite.css></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><div class=header-container><header class=header><nav class=nav><div class=logo><a href=https://shayan-p.github.io/ accesskey=h title="Shayan Pardis (Alt + H)">Shayan Pardis</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://shayan-p.github.io/about title=About><span>About</span></a></li><li><a href=https://shayan-p.github.io/cv.pdf title=CV><span>CV</span></a></li><li><a href=https://shayan-p.github.io/old title="Good Old Webite"><span>Good Old Webite</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://shayan-p.github.io/ideas title="Idea Book"><span>Idea Book</span></a></li></ul></nav></header></div><style>.header-container{position:sticky;top:0;z-index:1000;background-color:var(--theme);color:var(--header-text);padding:.5rem 1rem;box-shadow:0 0 10px var(--border);width:100%}</style><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://shayan-p.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://shayan-p.github.io/ideas/>Idea Book üìîüñäÔ∏è</a></div><h1 class="post-title entry-hint-parent">Dynamic Neural Memory for In-Context Learning: SSMs or Transformers?</h1><div class=post-meta><span title='2024-10-30 01:23:20 -0400 EDT'>October 30, 2024</span>&nbsp;¬∑&nbsp;Shayan Pardis</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=#motivation aria-label=Motivation>Motivation</a></li><li><a href=#related-work aria-label="Related Work">Related Work</a></li><li><a href=#dynamic-neural-memory aria-label="Dynamic Neural Memory">Dynamic Neural Memory</a></li><li><a href=#potential-issues aria-label="Potential Issues">Potential Issues</a></li><li><a href=#final-thoughts aria-label="Final Thoughts">Final Thoughts</a></li></ul><li><a href=#citation aria-label=Citation>Citation</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h3 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h3><p>This idea was inspired by a talk that Albert Gu gave at Simons Institute Workshop <a href=https://simons.berkeley.edu/workshops/transformers-computational-model>Transformers as a Computational Model</a> (<a href=https://simons.berkeley.edu/talks/albert-gu-carnegie-mellon-university-2024-09-27>link to the talk</a>).
While listening to this talk, I began to think about how we might design a new memory model by combining features from both State Space Models and transformers.</p><p>State Space Models
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#dblpjournals%2fcorr%2fabs-2111-00396><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Albert"><span itemprop=familyName>Gu</span></span>
 <em>et al.</em>, <span itemprop=datePublished>2021</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Gu</span>, 
<meta itemprop=givenName content="Albert">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Goel</span>, 
<meta itemprop=givenName content="Karan">K.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>R√©</span>, 
<meta itemprop=givenName content="Christopher">C.</span>
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name>Efficiently modeling long sequences with structured state spaces</span>.<i>
<span itemprop=about>CoRR</span>, abs/2111.00396</i>. Retrieved from 
<a href=https://arxiv.org/abs/2111.00396 itemprop=identifier itemtype=https://schema.org/URL>https://arxiv.org/abs/2111.00396</a></span>
</span></span>)</span>
compress all the history in a finite memory.
By contrast, transformers
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#dblpjournals%2fcorr%2fvaswanispujgkp17><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Ashish"><span itemprop=familyName>Vaswani</span></span>
 <em>et al.</em>, <span itemprop=datePublished>2017</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Vaswani</span>, 
<meta itemprop=givenName content="Ashish">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Shazeer</span>, 
<meta itemprop=givenName content="Noam">N.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Parmar</span>, 
<meta itemprop=givenName content="Niki">N.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Uszkoreit</span>, 
<meta itemprop=givenName content="Jakob">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Jones</span>, 
<meta itemprop=givenName content="Llion">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Gomez</span>, 
<meta itemprop=givenName content="Aidan N.">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kaiser</span>, 
<meta itemprop=givenName content="Lukasz">L.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Polosukhin</span>, 
<meta itemprop=givenName content="Illia">I.</span>
 
(<span itemprop=datePublished>2017</span>).
 <span itemprop=name>Attention is all you need</span>.<i>
<span itemprop=about>CoRR</span>, abs/1706.03762</i>. Retrieved from 
<a href=http://arxiv.org/abs/1706.03762 itemprop=identifier itemtype=https://schema.org/URL>http://arxiv.org/abs/1706.03762</a></span>
</span></span>)</span>
keep all the context. Intuitively, transformers function like a database with $O(n)$ memory growth, while SSMs focus on compressing the most critical information from the context into $O(1)$ memory.</p><p>Can we use the best of both worlds and design a graph-like memory model that dynamically adds new nodes (like transformers, but with sublinear growth) while also learning to compress information hierarchically into rich latent representations (like SSMs)?</p><h3 id=related-work>Related Work<a hidden class=anchor aria-hidden=true href=#related-work>#</a></h3><p>Human memory is associative; we remember concepts by their relationship to each other. <a href=https://en.wikipedia.org/wiki/Hopfield_network>Hopfield networks</a> provide one of the earliest examples of associative memory in artificial neural networks.
In Hopfield Network is All You Need
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#dblpjournals%2fcorr%2fabs-2008-02217><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Hubert"><span itemprop=familyName>Ramsauer</span></span>
 <em>et al.</em>, <span itemprop=datePublished>2020</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ramsauer</span>, 
<meta itemprop=givenName content="Hubert">H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Sch√§fl</span>, 
<meta itemprop=givenName content="Bernhard">B.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lehner</span>, 
<meta itemprop=givenName content="Johannes">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Seidl</span>, 
<meta itemprop=givenName content="Philipp">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Widrich</span>, 
<meta itemprop=givenName content="Michael">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Gruber</span>, 
<meta itemprop=givenName content="Lukas">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Holzleitner</span>, 
<meta itemprop=givenName content="Markus">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Pavlovic</span>, 
<meta itemprop=givenName content="Milena">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Sandve</span>, 
<meta itemprop=givenName content="Geir Kjetil">G.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Greiff</span>, 
<meta itemprop=givenName content="Victor">V.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kreil</span>, 
<meta itemprop=givenName content="David P.">D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kopp</span>, 
<meta itemprop=givenName content="Michael">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Klambauer</span>, 
<meta itemprop=givenName content="G√ºnter">G.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Brandstetter</span>, 
<meta itemprop=givenName content="Johannes">J.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Hochreiter</span>, 
<meta itemprop=givenName content="Sepp">S.</span>
 
(<span itemprop=datePublished>2020</span>).
 <span itemprop=name>Hopfield networks is all you need</span>.<i>
<span itemprop=about>CoRR</span>, abs/2008.02217</i>. Retrieved from 
<a href=https://arxiv.org/abs/2008.02217 itemprop=identifier itemtype=https://schema.org/URL>https://arxiv.org/abs/2008.02217</a></span>
</span></span>)</span>
Hopfield layers are introduced and their convergence and capacity properties are analyzed.</p><p>In Memory Networks
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#weston2015memorynetworks><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jason"><span itemprop=familyName>Weston</span></span>
 <em>et al.</em>, <span itemprop=datePublished>2015</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/CreativeWork data-type=default><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Weston</span>, 
<meta itemprop=givenName content="Jason">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chopra</span>, 
<meta itemprop=givenName content="Sumit">S.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Bordes</span>, 
<meta itemprop=givenName content="Antoine">A.</span>
 
(<span itemprop=datePublished>2015</span>).
 <span itemprop=name><i>Memory networks</i></span>.
 Retrieved from 
<a href=https://arxiv.org/abs/1410.3916 itemprop=identifier itemtype=https://schema.org/URL>https://arxiv.org/abs/1410.3916</a></span>
</span></span>)</span>
a memory design with four components is introduced:</p><ul><li>feature map $I$: this encodes the data in the latent space</li><li>generalization component $G$: updates the memory with the new data</li><li>output feature map $O$: produces output based on query</li><li>response component $R$: this is essentially the decoder</li></ul><p>This is how the output feature map operates:
$$O(x, m) = \argmax_{i=1,&mldr;,N} s_O(x, m_i)$$
where $m_i$ are the memory elements, and $x$ is the featurized query, and $s_O$ is a score function. The score function they proposed was:
$$
s_O(x, y) = \Phi_x(x)^T U^T U \Phi_y(y)
$$</p><p>Interestingly, this is conceptually similar to the attention mechanism in which we assign score $x^t \frac{Q^T K}{d_k} y$ to a feature pair $x, y$. In this context, $U$ from above can be derived through <a href=https://en.wikipedia.org/wiki/Cholesky_decomposition>Cholesky decomposition</a> and the softmax function in attention replaces argmax in this formulation.</p><h3 id=dynamic-neural-memory>Dynamic Neural Memory<a hidden class=anchor aria-hidden=true href=#dynamic-neural-memory>#</a></h3><p>Can we combine the best of both worlds: the compression mechanism of SSMs and the dynamic, database-like nature of transformers, to design a memory mechanism that grows sublinearly, perhaps $O(\log n)$ with respect to input size? This would not only address transformers‚Äô context length limitation but also provide a balance between <strong>understanding/compressing</strong> and <strong>memorizing</strong>. Essentially, this is a tradeoff between <strong>memory</strong> and <strong>runtime search</strong>.</p><p>It&rsquo;s very natural to think of memory as a graph of concepts where each concept has its own embedding.</p><p>Adding new concepts involves traversing this graph, adding new nodes and edges, and enriching the latent representations of existing nodes. Retrieving information would involve a similar traversal, using the encoded query in the latent space and aggregating features from the visited nodes.</p><p>A simple first step could be to design a neural <a href=https://en.wikipedia.org/wiki/Binary_search_tree>binary search tree</a>. In this design, each node has three learnable, parameterized functions: <code>update</code>, <code>stop</code>, and <code>router</code>. Adding new data could look like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>add_concept</span><span class=p>(</span><span class=n>memory_node</span><span class=p>,</span> <span class=n>concept</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>update</span><span class=p>(</span><span class=n>memory_node</span><span class=p>,</span> <span class=n>concept</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>stop</span><span class=p>(</span><span class=n>memory_node</span><span class=p>,</span> <span class=n>concept</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span>
</span></span><span class=line><span class=cl>    <span class=n>next_memory_node</span> <span class=o>=</span> <span class=n>router</span><span class=p>(</span><span class=n>memory_node</span><span class=p>,</span> <span class=n>concept</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>add_concept</span><span class=p>(</span><span class=n>memory_node</span><span class=p>,</span> <span class=n>concept</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>new_info</span> <span class=o>=</span> <span class=n>encode</span><span class=p>(</span><span class=n>information</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>add_concept</span><span class=p>(</span><span class=n>memory_node</span><span class=o>=</span><span class=n>memory</span><span class=o>.</span><span class=n>root</span><span class=p>,</span> <span class=n>concept</span><span class=o>=</span><span class=n>new_info</span><span class=p>)</span>
</span></span></code></pre></div><p>In a graph-based structure, router could be implemented as an attention block. Rather than routing to a single node, it could calculate a probability distribution over adjacent nodes and pass the probability flow down the graph.</p><h3 id=potential-issues>Potential Issues<a hidden class=anchor aria-hidden=true href=#potential-issues>#</a></h3><ul><li><strong>Learnability</strong>: How do we define a learning mechanism that ensures convergence? In particular, designing an objective function for training may be complex due to the non-linear, high-dimensional updates involved in memory storage and retrieval.</li><li><strong>Scalability</strong>: The process described above is sequential. How can we limit the number of sequential steps required for memory updates, or find a balance between traversal depth (i.e., <strong>&ldquo;thinking time&rdquo;</strong>) and <strong>accuracy</strong>?</li></ul><h3 id=final-thoughts>Final Thoughts<a hidden class=anchor aria-hidden=true href=#final-thoughts>#</a></h3><p>The attention mechanism, at its core, functions as a read-memory operation. It became a breakthrough for deep learning due to its generality and scalability. However, attention-based memory operates more like a database and less like natural memory, which is hierarchically organized and capable of efficient retrieval based on relationships between concepts.</p><p>An improved memory design could offer hierarchical data storage, enhance generalization, improve data efficiency, and overcome the context length limitations in transformers. These advancements could significantly impact how we approach sequential problem-solving in NLP and beyond.</p><p>One motivating implication for me is in-context learning. Think of the memory model in world models
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#https%2f%2fdoi.org%2f10.5281%2fzenodo.1207631><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="David"><span itemprop=familyName>Ha</span></span>
 & 
<span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="J√ºrgen"><span itemprop=familyName>Schmidhuber</span></span>, <span itemprop=datePublished>2018</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ha</span>, 
<meta itemprop=givenName content="David">D.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Schmidhuber</span>, 
<meta itemprop=givenName content="J√ºrgen">J.</span>
 
(<span itemprop=datePublished>2018</span>).
 <span itemprop=name>World models</span>.
<a href=https://doi.org/10.5281/ZENODO.1207631 itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.5281/ZENODO.1207631</a></span>
</span></span>)</span>
. As an agent interacts with its environment, it receives a continuous stream of data, which it uses to build a representation of the environment. The effectiveness of the memory model determines how well the agent learns from past experiences and assigns credit to rewards. Integrating memory models into reinforcement learning has been explored in works such as Reinforcement Learning as One Big Sequence Modeling Problem
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#dblpjournals%2fcorr%2fabs-2106-02039><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Michael"><span itemprop=familyName>Janner</span></span>
 <em>et al.</em>, <span itemprop=datePublished>2021</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Janner</span>, 
<meta itemprop=givenName content="Michael">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, 
<meta itemprop=givenName content="Qiyang">Q.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Levine</span>, 
<meta itemprop=givenName content="Sergey">S.</span>
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name>Reinforcement learning as one big sequence modeling problem</span>.<i>
<span itemprop=about>CoRR</span>, abs/2106.02039</i>. Retrieved from 
<a href=https://arxiv.org/abs/2106.02039 itemprop=identifier itemtype=https://schema.org/URL>https://arxiv.org/abs/2106.02039</a></span>
</span></span>)</span>
and Decision Transformer: Reinforcement Learning via Sequence Modeling
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#chen2021decisiontransformerreinforcementlearning><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Lili"><span itemprop=familyName>Chen</span></span>
 <em>et al.</em>, <span itemprop=datePublished>2021</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/CreativeWork data-type=default><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chen</span>, 
<meta itemprop=givenName content="Lili">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lu</span>, 
<meta itemprop=givenName content="Kevin">K.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rajeswaran</span>, 
<meta itemprop=givenName content="Aravind">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lee</span>, 
<meta itemprop=givenName content="Kimin">K.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Grover</span>, 
<meta itemprop=givenName content="Aditya">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Laskin</span>, 
<meta itemprop=givenName content="Michael">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Abbeel</span>, 
<meta itemprop=givenName content="Pieter">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Srinivas</span>, 
<meta itemprop=givenName content="Aravind">A.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Mordatch</span>, 
<meta itemprop=givenName content="Igor">I.</span>
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name><i>Decision transformer: Reinforcement learning via sequence modeling</i></span>.
 Retrieved from 
<a href=https://arxiv.org/abs/2106.01345 itemprop=identifier itemtype=https://schema.org/URL>https://arxiv.org/abs/2106.01345</a></span>
</span></span>)</span>
.</p><h2 class=hugo-heading id=citation>Citation</h2><p><strong>Cited as:</strong></p><p>Shayan Pardis. (Oct 2024). Dynamic Neural Memory for In-Context Learning: SSMs or Transformers?. <a href=https://shayan-p.github.io/ideas/neural-memory-for-in-context-learning/>https://shayan-p.github.io/ideas/neural-memory-for-in-context-learning/</a>.</p>Or<div class=highlight><pre class=chroma><code class=language-json data-lang=json id=cite-me><span class=line><span class=cl><span class=err>@article</span><span class=p>{</span> <span class=err>shayan-pardis2024dynamic-neural-memory-for-in-context-learning-ssms-or-transformers,</span>
</span></span><span class=line><span class=cl>  <span class=err>title</span>   <span class=err>=</span> <span class=nt>"Dynamic Neural Memory for In-Context Learning: SSMs or Transformers?"</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=err>author</span>  <span class=err>=</span> <span class=nt>"Shayan Pardis"</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=err>year</span>    <span class=err>=</span> <span class=nt>"2024"</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=err>month</span>   <span class=err>=</span> <span class=nt>"Oct"</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=err>url</span>     <span class=err>=</span> <span class=nt>"https://shayan-p.github.io/ideas/neural-memory-for-in-context-learning/"</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre><button class=copy-code onclick=copyToClipboard()>copy</button>
<script>function copyToClipboard(){const e=document.querySelector("#cite-me"),t=e.innerText;navigator.clipboard.writeText(t).then(function(){alert("Citation copied to clipboard!")},function(e){console.error("Could not copy text: ",e)})}</script></div><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><section class=hugo-cite-bibliography><dl><div id=dblpjournals/corr/vaswanispujgkp17 style=width:100%><dt style=width:100%>[1]
<span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Ashish"><span itemprop=familyName>Vaswani</span></span>
 <em>et al.</em>
<a href=http://arxiv.org/abs/1706.03762 target=_blank>Attention is all you need</a>
CoRR
(2017)</dt><dd style=width:100%><span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Vaswani</span>, 
<meta itemprop=givenName content="Ashish">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Shazeer</span>, 
<meta itemprop=givenName content="Noam">N.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Parmar</span>, 
<meta itemprop=givenName content="Niki">N.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Uszkoreit</span>, 
<meta itemprop=givenName content="Jakob">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Jones</span>, 
<meta itemprop=givenName content="Llion">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Gomez</span>, 
<meta itemprop=givenName content="Aidan N.">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kaiser</span>, 
<meta itemprop=givenName content="Lukasz">L.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Polosukhin</span>, 
<meta itemprop=givenName content="Illia">I.</span>
 
(<span itemprop=datePublished>2017</span>).
 <span itemprop=name>Attention is all you need</span>.<i>
<span itemprop=about>CoRR</span>, abs/1706.03762</i>. Retrieved from 
<a href=http://arxiv.org/abs/1706.03762 itemprop=identifier itemtype=https://schema.org/URL>http://arxiv.org/abs/1706.03762</a></span></dd></div><div id=dblpjournals/corr/abs-2008-02217 style=width:100%><dt style=width:100%>[2]
<span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Hubert"><span itemprop=familyName>Ramsauer</span></span>
 <em>et al.</em>
<a href=https://arxiv.org/abs/2008.02217 target=_blank>Hopfield networks is all you need</a>
CoRR
(2020)</dt><dd style=width:100%><span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ramsauer</span>, 
<meta itemprop=givenName content="Hubert">H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Sch√§fl</span>, 
<meta itemprop=givenName content="Bernhard">B.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lehner</span>, 
<meta itemprop=givenName content="Johannes">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Seidl</span>, 
<meta itemprop=givenName content="Philipp">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Widrich</span>, 
<meta itemprop=givenName content="Michael">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Gruber</span>, 
<meta itemprop=givenName content="Lukas">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Holzleitner</span>, 
<meta itemprop=givenName content="Markus">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Pavlovic</span>, 
<meta itemprop=givenName content="Milena">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Sandve</span>, 
<meta itemprop=givenName content="Geir Kjetil">G.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Greiff</span>, 
<meta itemprop=givenName content="Victor">V.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kreil</span>, 
<meta itemprop=givenName content="David P.">D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kopp</span>, 
<meta itemprop=givenName content="Michael">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Klambauer</span>, 
<meta itemprop=givenName content="G√ºnter">G.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Brandstetter</span>, 
<meta itemprop=givenName content="Johannes">J.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Hochreiter</span>, 
<meta itemprop=givenName content="Sepp">S.</span>
 
(<span itemprop=datePublished>2020</span>).
 <span itemprop=name>Hopfield networks is all you need</span>.<i>
<span itemprop=about>CoRR</span>, abs/2008.02217</i>. Retrieved from 
<a href=https://arxiv.org/abs/2008.02217 itemprop=identifier itemtype=https://schema.org/URL>https://arxiv.org/abs/2008.02217</a></span></dd></div><div id=dblpjournals/corr/abs-2106-02039 style=width:100%><dt style=width:100%>[3]
<span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Michael"><span itemprop=familyName>Janner</span></span>
 <em>et al.</em>
<a href=https://arxiv.org/abs/2106.02039 target=_blank>Reinforcement learning as one big sequence modeling problem</a>
CoRR
(2021)</dt><dd style=width:100%><span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Janner</span>, 
<meta itemprop=givenName content="Michael">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, 
<meta itemprop=givenName content="Qiyang">Q.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Levine</span>, 
<meta itemprop=givenName content="Sergey">S.</span>
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name>Reinforcement learning as one big sequence modeling problem</span>.<i>
<span itemprop=about>CoRR</span>, abs/2106.02039</i>. Retrieved from 
<a href=https://arxiv.org/abs/2106.02039 itemprop=identifier itemtype=https://schema.org/URL>https://arxiv.org/abs/2106.02039</a></span></dd></div><div id=dblpjournals/corr/abs-2111-00396 style=width:100%><dt style=width:100%>[4]
<span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Albert"><span itemprop=familyName>Gu</span></span>
 <em>et al.</em>
<a href=https://arxiv.org/abs/2111.00396 target=_blank>Efficiently modeling long sequences with structured state spaces</a>
CoRR
(2021)</dt><dd style=width:100%><span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Gu</span>, 
<meta itemprop=givenName content="Albert">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Goel</span>, 
<meta itemprop=givenName content="Karan">K.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>R√©</span>, 
<meta itemprop=givenName content="Christopher">C.</span>
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name>Efficiently modeling long sequences with structured state spaces</span>.<i>
<span itemprop=about>CoRR</span>, abs/2111.00396</i>. Retrieved from 
<a href=https://arxiv.org/abs/2111.00396 itemprop=identifier itemtype=https://schema.org/URL>https://arxiv.org/abs/2111.00396</a></span></dd></div><div id=chen2021decisiontransformerreinforcementlearning style=width:100%><dt style=width:100%>[5]
<span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Lili"><span itemprop=familyName>Chen</span></span>
 <em>et al.</em>
<a href=https://arxiv.org/abs/2106.01345 target=_blank>Decision transformer: Reinforcement learning via sequence modeling</a>
(2021)</dt><dd style=width:100%><span itemscope itemtype=https://schema.org/CreativeWork data-type=default><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chen</span>, 
<meta itemprop=givenName content="Lili">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lu</span>, 
<meta itemprop=givenName content="Kevin">K.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rajeswaran</span>, 
<meta itemprop=givenName content="Aravind">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lee</span>, 
<meta itemprop=givenName content="Kimin">K.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Grover</span>, 
<meta itemprop=givenName content="Aditya">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Laskin</span>, 
<meta itemprop=givenName content="Michael">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Abbeel</span>, 
<meta itemprop=givenName content="Pieter">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Srinivas</span>, 
<meta itemprop=givenName content="Aravind">A.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Mordatch</span>, 
<meta itemprop=givenName content="Igor">I.</span>
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name><i>Decision transformer: Reinforcement learning via sequence modeling</i></span>.
 Retrieved from 
<a href=https://arxiv.org/abs/2106.01345 itemprop=identifier itemtype=https://schema.org/URL>https://arxiv.org/abs/2106.01345</a></span></dd></div><div id=https//doi.org/10.5281/zenodo.1207631 style=width:100%><dt style=width:100%>[6]
<span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="David"><span itemprop=familyName>Ha</span></span>
 & 
<span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="J√ºrgen"><span itemprop=familyName>Schmidhuber</span></span>
<a href=https://zenodo.org/record/1207631 target=_blank>World models</a>
(2018)</dt><dd style=width:100%><span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ha</span>, 
<meta itemprop=givenName content="David">D.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Schmidhuber</span>, 
<meta itemprop=givenName content="J√ºrgen">J.</span>
 
(<span itemprop=datePublished>2018</span>).
 <span itemprop=name>World models</span>.
<a href=https://doi.org/10.5281/ZENODO.1207631 itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.5281/ZENODO.1207631</a></span></dd></div><div id=weston2015memorynetworks style=width:100%><dt style=width:100%>[7]
<span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jason"><span itemprop=familyName>Weston</span></span>
 <em>et al.</em>
<a href=https://arxiv.org/abs/1410.3916 target=_blank>Memory networks</a>
(2015)</dt><dd style=width:100%><span itemscope itemtype=https://schema.org/CreativeWork data-type=default><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Weston</span>, 
<meta itemprop=givenName content="Jason">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chopra</span>, 
<meta itemprop=givenName content="Sumit">S.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Bordes</span>, 
<meta itemprop=givenName content="Antoine">A.</span>
 
(<span itemprop=datePublished>2015</span>).
 <span itemprop=name><i>Memory networks</i></span>.
 Retrieved from 
<a href=https://arxiv.org/abs/1410.3916 itemprop=identifier itemtype=https://schema.org/URL>https://arxiv.org/abs/1410.3916</a></span></dd></div></dl></section></div><footer class=post-footer><ul class=post-tags><li><a href=https://shayan-p.github.io/tags/nlp/>NLP</a></li><li><a href=https://shayan-p.github.io/tags/model-architecture/>Model Architecture</a></li></ul></footer></article></main><footer class=footer><span>¬© Shayan Pardis</span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>