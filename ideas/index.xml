<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Idea Book üìîüñäÔ∏è on Shayan Pardis</title>
    <link>https://www.shayanp.me/ideas/</link>
    <description>Recent content in Idea Book üìîüñäÔ∏è on Shayan Pardis</description>
    <generator>Hugo -- 0.138.0</generator>
    <language>en</language>
    <copyright>Shayan Pardis</copyright>
    <lastBuildDate>Wed, 30 Oct 2024 01:23:20 -0400</lastBuildDate>
    <atom:link href="https://www.shayanp.me/ideas/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Dynamic Neural Memory for In-Context Learning: SSMs or Transformers?</title>
      <link>https://www.shayanp.me/ideas/neural-memory-for-in-context-learning/</link>
      <pubDate>Wed, 30 Oct 2024 01:23:20 -0400</pubDate>
      <guid>https://www.shayanp.me/ideas/neural-memory-for-in-context-learning/</guid>
      <description>&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;This idea was inspired by a talk that Albert Gu gave at Simons Institute Workshop &lt;a href=&#34;https://simons.berkeley.edu/workshops/transformers-computational-model&#34;&gt;Transformers as a Computational Model&lt;/a&gt; (&lt;a href=&#34;https://simons.berkeley.edu/talks/albert-gu-carnegie-mellon-university-2024-09-27&#34;&gt;link to the talk&lt;/a&gt;).
While listening to this talk, I began to think about how we might design a new memory model by combining features from both State Space Models and transformers.&lt;/p&gt;
&lt;p&gt;State Space Models 




&lt;span class=&#34;hugo-cite-intext&#34;
        itemprop=&#34;citation&#34;&gt;(&lt;span class=&#34;hugo-cite-group&#34;&gt;

          &lt;a href=&#34;#dblpjournals%2fcorr%2fabs-2111-00396&#34;&gt;&lt;span class=&#34;visually-hidden&#34;&gt;Citation: &lt;/span&gt;
              &lt;span itemprop=&#34;author&#34; itemscope itemtype=&#34;https://schema.org/Person&#34;&gt;&lt;meta itemprop=&#34;givenName&#34; content=&#34;Albert&#34;&gt;&lt;span itemprop=&#34;familyName&#34;&gt;Gu&lt;/span&gt;&lt;/span&gt;
              &amp;#32;&lt;em&gt;et al.&lt;/em&gt;,&amp;#32;&lt;span itemprop=&#34;datePublished&#34;&gt;2021&lt;/span&gt;&lt;/a&gt;&lt;span class=&#34;hugo-cite-citation&#34;&gt; 










&lt;span itemscope
      itemtype=&#34;https://schema.org/Article&#34;
      data-type=&#34;article&#34;&gt;&lt;span itemprop=&#34;author&#34; itemscope itemtype=&#34;https://schema.org/Person&#34;&gt;&lt;span itemprop=&#34;familyName&#34;&gt;Gu&lt;/span&gt;,&amp;#32;
    &lt;meta itemprop=&#34;givenName&#34; content=&#34;Albert&#34; /&gt;
    A.&lt;/span&gt;,&amp;#32;
  &lt;span itemprop=&#34;author&#34; itemscope itemtype=&#34;https://schema.org/Person&#34;&gt;&lt;span itemprop=&#34;familyName&#34;&gt;Goel&lt;/span&gt;,&amp;#32;
    &lt;meta itemprop=&#34;givenName&#34; content=&#34;Karan&#34; /&gt;
    K.&lt;/span&gt;&amp;#32;&amp;amp;&amp;#32;&lt;span itemprop=&#34;author&#34; itemscope itemtype=&#34;https://schema.org/Person&#34;&gt;&lt;span itemprop=&#34;familyName&#34;&gt;R√©&lt;/span&gt;,&amp;#32;
    &lt;meta itemprop=&#34;givenName&#34; content=&#34;Christopher&#34; /&gt;
    C.&lt;/span&gt;
  &amp;#32;
    (&lt;span itemprop=&#34;datePublished&#34;&gt;2021&lt;/span&gt;).
  &amp;#32;&lt;span itemprop=&#34;name&#34;&gt;Efficiently modeling long sequences with structured state spaces&lt;/span&gt;.&lt;i&gt;
    &lt;span itemprop=&#34;about&#34;&gt;CoRR&lt;/span&gt;,&amp;#32;abs/2111.00396&lt;/i&gt;.&amp;#32;Retrieved from&amp;#32;
  &lt;a href=&#34;https://arxiv.org/abs/2111.00396&#34;
     itemprop=&#34;identifier&#34;
     itemtype=&#34;https://schema.org/URL&#34;&gt;https://arxiv.org/abs/2111.00396&lt;/a&gt;&lt;/span&gt;




&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
 compress all the history in a finite memory.
By contrast, transformers 




&lt;span class=&#34;hugo-cite-intext&#34;
        itemprop=&#34;citation&#34;&gt;(&lt;span class=&#34;hugo-cite-group&#34;&gt;

          &lt;a href=&#34;#dblpjournals%2fcorr%2fvaswanispujgkp17&#34;&gt;&lt;span class=&#34;visually-hidden&#34;&gt;Citation: &lt;/span&gt;
              &lt;span itemprop=&#34;author&#34; itemscope itemtype=&#34;https://schema.org/Person&#34;&gt;&lt;meta itemprop=&#34;givenName&#34; content=&#34;Ashish&#34;&gt;&lt;span itemprop=&#34;familyName&#34;&gt;Vaswani&lt;/span&gt;&lt;/span&gt;
              &amp;#32;&lt;em&gt;et al.&lt;/em&gt;,&amp;#32;&lt;span itemprop=&#34;datePublished&#34;&gt;2017&lt;/span&gt;&lt;/a&gt;&lt;span class=&#34;hugo-cite-citation&#34;&gt; 










&lt;span itemscope
      itemtype=&#34;https://schema.org/Article&#34;
      data-type=&#34;article&#34;&gt;&lt;span itemprop=&#34;author&#34; itemscope itemtype=&#34;https://schema.org/Person&#34;&gt;&lt;span itemprop=&#34;familyName&#34;&gt;Vaswani&lt;/span&gt;,&amp;#32;
    &lt;meta itemprop=&#34;givenName&#34; content=&#34;Ashish&#34; /&gt;
    A.&lt;/span&gt;,&amp;#32;
  &lt;span itemprop=&#34;author&#34; itemscope itemtype=&#34;https://schema.org/Person&#34;&gt;&lt;span itemprop=&#34;familyName&#34;&gt;Shazeer&lt;/span&gt;,&amp;#32;
    &lt;meta itemprop=&#34;givenName&#34; content=&#34;Noam&#34; /&gt;
    N.&lt;/span&gt;,&amp;#32;
  &lt;span itemprop=&#34;author&#34; itemscope itemtype=&#34;https://schema.org/Person&#34;&gt;&lt;span itemprop=&#34;familyName&#34;&gt;Parmar&lt;/span&gt;,&amp;#32;
    &lt;meta itemprop=&#34;givenName&#34; content=&#34;Niki&#34; /&gt;
    N.&lt;/span&gt;,&amp;#32;
  &lt;span itemprop=&#34;author&#34; itemscope itemtype=&#34;https://schema.org/Person&#34;&gt;&lt;span itemprop=&#34;familyName&#34;&gt;Uszkoreit&lt;/span&gt;,&amp;#32;
    &lt;meta itemprop=&#34;givenName&#34; content=&#34;Jakob&#34; /&gt;
    J.&lt;/span&gt;,&amp;#32;
  &lt;span itemprop=&#34;author&#34; itemscope itemtype=&#34;https://schema.org/Person&#34;&gt;&lt;span itemprop=&#34;familyName&#34;&gt;Jones&lt;/span&gt;,&amp;#32;
    &lt;meta itemprop=&#34;givenName&#34; content=&#34;Llion&#34; /&gt;
    L.&lt;/span&gt;,&amp;#32;
  &lt;span itemprop=&#34;author&#34; itemscope itemtype=&#34;https://schema.org/Person&#34;&gt;&lt;span itemprop=&#34;familyName&#34;&gt;Gomez&lt;/span&gt;,&amp;#32;
    &lt;meta itemprop=&#34;givenName&#34; content=&#34;Aidan N.&#34; /&gt;
    A.&lt;/span&gt;,&amp;#32;
  &lt;span itemprop=&#34;author&#34; itemscope itemtype=&#34;https://schema.org/Person&#34;&gt;&lt;span itemprop=&#34;familyName&#34;&gt;Kaiser&lt;/span&gt;,&amp;#32;
    &lt;meta itemprop=&#34;givenName&#34; content=&#34;Lukasz&#34; /&gt;
    L.&lt;/span&gt;&amp;#32;&amp;amp;&amp;#32;&lt;span itemprop=&#34;author&#34; itemscope itemtype=&#34;https://schema.org/Person&#34;&gt;&lt;span itemprop=&#34;familyName&#34;&gt;Polosukhin&lt;/span&gt;,&amp;#32;
    &lt;meta itemprop=&#34;givenName&#34; content=&#34;Illia&#34; /&gt;
    I.&lt;/span&gt;
  &amp;#32;
    (&lt;span itemprop=&#34;datePublished&#34;&gt;2017&lt;/span&gt;).
  &amp;#32;&lt;span itemprop=&#34;name&#34;&gt;Attention is all you need&lt;/span&gt;.&lt;i&gt;
    &lt;span itemprop=&#34;about&#34;&gt;CoRR&lt;/span&gt;,&amp;#32;abs/1706.03762&lt;/i&gt;.&amp;#32;Retrieved from&amp;#32;
  &lt;a href=&#34;http://arxiv.org/abs/1706.03762&#34;
     itemprop=&#34;identifier&#34;
     itemtype=&#34;https://schema.org/URL&#34;&gt;http://arxiv.org/abs/1706.03762&lt;/a&gt;&lt;/span&gt;




&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
 keep all the context. Intuitively, transformers function like a database with $O(n)$ memory growth, while SSMs focus on compressing the most critical information from the context into $O(1)$ memory.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
