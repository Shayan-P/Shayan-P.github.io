[{"content":"Motivation This idea was inspired by a talk that Albert Gu gave at Simons Institute Workshop Transformers as a Computational Model (link to the talk). While listening to this talk, I began to think about how we might design a new memory model by combining features from both State Space Models and transformers.\nState Space Models ( Citation: Gu \u0026#32;et al.,\u0026#32;2021 Gu,\u0026#32; A.,\u0026#32; Goel,\u0026#32; K.\u0026#32;\u0026amp;\u0026#32;Ré,\u0026#32; C. \u0026#32; (2021). \u0026#32;Efficiently modeling long sequences with structured state spaces. CoRR,\u0026#32;abs/2111.00396.\u0026#32;Retrieved from\u0026#32; https://arxiv.org/abs/2111.00396 ) compress all the history in a finite memory. By contrast, transformers ( Citation: Vaswani \u0026#32;et al.,\u0026#32;2017 Vaswani,\u0026#32; A.,\u0026#32; Shazeer,\u0026#32; N.,\u0026#32; Parmar,\u0026#32; N.,\u0026#32; Uszkoreit,\u0026#32; J.,\u0026#32; Jones,\u0026#32; L.,\u0026#32; Gomez,\u0026#32; A.,\u0026#32; Kaiser,\u0026#32; L.\u0026#32;\u0026amp;\u0026#32;Polosukhin,\u0026#32; I. \u0026#32; (2017). \u0026#32;Attention is all you need. CoRR,\u0026#32;abs/1706.03762.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1706.03762 ) keep all the context. Intuitively, transformers function like a database with $O(n)$ memory growth, while SSMs focus on compressing the most critical information from the context into $O(1)$ memory.\nCan we use the best of both worlds and design a graph-like memory model that dynamically adds new nodes (like transformers, but with sublinear growth) while also learning to compress information hierarchically into rich latent representations (like SSMs)?\nRelated Work Human memory is associative; we remember concepts by their relationship to each other. Hopfield networks provide one of the earliest examples of associative memory in artificial neural networks. In Hopfield Network is All You Need ( Citation: Ramsauer \u0026#32;et al.,\u0026#32;2020 Ramsauer,\u0026#32; H.,\u0026#32; Schäfl,\u0026#32; B.,\u0026#32; Lehner,\u0026#32; J.,\u0026#32; Seidl,\u0026#32; P.,\u0026#32; Widrich,\u0026#32; M.,\u0026#32; Gruber,\u0026#32; L.,\u0026#32; Holzleitner,\u0026#32; M.,\u0026#32; Pavlovic,\u0026#32; M.,\u0026#32; Sandve,\u0026#32; G.,\u0026#32; Greiff,\u0026#32; V.,\u0026#32; Kreil,\u0026#32; D.,\u0026#32; Kopp,\u0026#32; M.,\u0026#32; Klambauer,\u0026#32; G.,\u0026#32; Brandstetter,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Hochreiter,\u0026#32; S. \u0026#32; (2020). \u0026#32;Hopfield networks is all you need. CoRR,\u0026#32;abs/2008.02217.\u0026#32;Retrieved from\u0026#32; https://arxiv.org/abs/2008.02217 ) Hopfield layers are introduced and their convergence and capacity properties are analyzed.\nIn Memory Networks ( Citation: Weston \u0026#32;et al.,\u0026#32;2015 Weston,\u0026#32; J.,\u0026#32; Chopra,\u0026#32; S.\u0026#32;\u0026amp;\u0026#32;Bordes,\u0026#32; A. \u0026#32; (2015). \u0026#32; Memory networks. \u0026#32;Retrieved from\u0026#32; https://arxiv.org/abs/1410.3916 ) a memory design with four components is introduced:\nfeature map $I$: this encodes the data in the latent space generalization component $G$: updates the memory with the new data output feature map $O$: produces output based on query response component $R$: this is essentially the decoder This is how the output feature map operates: $$O(x, m) = \\argmax_{i=1,\u0026hellip;,N} s_O(x, m_i)$$ where $m_i$ are the memory elements, and $x$ is the featurized query, and $s_O$ is a score function. The score function they proposed was: $$ s_O(x, y) = \\Phi_x(x)^T U^T U \\Phi_y(y) $$\nInterestingly, this is conceptually similar to the attention mechanism in which we assign score $x^t \\frac{Q^T K}{d_k} y$ to a feature pair $x, y$. In this context, $U$ from above can be derived through Cholesky decomposition and the softmax function in attention replaces argmax in this formulation.\nDynamic Neural Memory Can we combine the best of both worlds: the compression mechanism of SSMs and the dynamic, database-like nature of transformers, to design a memory mechanism that grows sublinearly, perhaps $O(\\log n)$ with respect to input size? This would not only address transformers’ context length limitation but also provide a balance between understanding/compressing and memorizing. Essentially, this is a tradeoff between memory and runtime search.\nIt\u0026rsquo;s very natural to think of memory as a graph of concepts where each concept has its own embedding.\nAdding new concepts involves traversing this graph, adding new nodes and edges, and enriching the latent representations of existing nodes. Retrieving information would involve a similar traversal, using the encoded query in the latent space and aggregating features from the visited nodes.\nA simple first step could be to design a neural binary search tree. In this design, each node has three learnable, parameterized functions: update, stop, and router. Adding new data could look like this:\ndef add_concept(memory_node, concept): update(memory_node, concept) if stop(memory_node, concept): return next_memory_node = router(memory_node, concept) add_concept(memory_node, concept) new_info = encode(information) add_concept(memory_node=memory.root, concept=new_info) In a graph-based structure, router could be implemented as an attention block. Rather than routing to a single node, it could calculate a probability distribution over adjacent nodes and pass the probability flow down the graph.\nPotential Issues Learnability: How do we define a learning mechanism that ensures convergence? In particular, designing an objective function for training may be complex due to the non-linear, high-dimensional updates involved in memory storage and retrieval. Scalability: The process described above is sequential. How can we limit the number of sequential steps required for memory updates, or find a balance between traversal depth (i.e., \u0026ldquo;thinking time\u0026rdquo;) and accuracy? Final Thoughts The attention mechanism, at its core, functions as a read-memory operation. It became a breakthrough for deep learning due to its generality and scalability. However, attention-based memory operates more like a database and less like natural memory, which is hierarchically organized and capable of efficient retrieval based on relationships between concepts.\nAn improved memory design could offer hierarchical data storage, enhance generalization, improve data efficiency, and overcome the context length limitations in transformers. These advancements could significantly impact how we approach sequential problem-solving in NLP and beyond.\nOne motivating implication for me is in-context learning. Think of the memory model in world models ( Citation: Ha \u0026#32;\u0026amp;\u0026#32; Schmidhuber,\u0026#32;2018 Ha,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Schmidhuber,\u0026#32; J. \u0026#32; (2018). \u0026#32;World models. https://doi.org/10.5281/ZENODO.1207631 ) . As an agent interacts with its environment, it receives a continuous stream of data, which it uses to build a representation of the environment. The effectiveness of the memory model determines how well the agent learns from past experiences and assigns credit to rewards. Integrating memory models into reinforcement learning has been explored in works such as Reinforcement Learning as One Big Sequence Modeling Problem ( Citation: Janner \u0026#32;et al.,\u0026#32;2021 Janner,\u0026#32; M.,\u0026#32; Li,\u0026#32; Q.\u0026#32;\u0026amp;\u0026#32;Levine,\u0026#32; S. \u0026#32; (2021). \u0026#32;Reinforcement learning as one big sequence modeling problem. CoRR,\u0026#32;abs/2106.02039.\u0026#32;Retrieved from\u0026#32; https://arxiv.org/abs/2106.02039 ) and Decision Transformer: Reinforcement Learning via Sequence Modeling ( Citation: Chen \u0026#32;et al.,\u0026#32;2021 Chen,\u0026#32; L.,\u0026#32; Lu,\u0026#32; K.,\u0026#32; Rajeswaran,\u0026#32; A.,\u0026#32; Lee,\u0026#32; K.,\u0026#32; Grover,\u0026#32; A.,\u0026#32; Laskin,\u0026#32; M.,\u0026#32; Abbeel,\u0026#32; P.,\u0026#32; Srinivas,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Mordatch,\u0026#32; I. \u0026#32; (2021). \u0026#32; Decision transformer: Reinforcement learning via sequence modeling. \u0026#32;Retrieved from\u0026#32; https://arxiv.org/abs/2106.01345 ) .\nCitation Cited as:\nShayan Pardis. (Oct 2024). Dynamic Neural Memory for In-Context Learning: SSMs or Transformers?. https://shayanp.me/ideas/neural-memory-for-in-context-learning/.\nOr @article{ shayan-pardis2024dynamic-neural-memory-for-in-context-learning-ssms-or-transformers, title = \"Dynamic Neural Memory for In-Context Learning: SSMs or Transformers?\", author = \"Shayan Pardis\", year = \"2024\", month = \"Oct\", url = \"https://shayanp.me/ideas/neural-memory-for-in-context-learning/\" } copy References [1] Vaswani \u0026#32;et al. Attention is all you need CoRR (2017) Vaswani,\u0026#32; A.,\u0026#32; Shazeer,\u0026#32; N.,\u0026#32; Parmar,\u0026#32; N.,\u0026#32; Uszkoreit,\u0026#32; J.,\u0026#32; Jones,\u0026#32; L.,\u0026#32; Gomez,\u0026#32; A.,\u0026#32; Kaiser,\u0026#32; L.\u0026#32;\u0026amp;\u0026#32;Polosukhin,\u0026#32; I. \u0026#32; (2017). \u0026#32;Attention is all you need. CoRR,\u0026#32;abs/1706.03762.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1706.03762 [2] Ramsauer \u0026#32;et al. Hopfield networks is all you need CoRR (2020) Ramsauer,\u0026#32; H.,\u0026#32; Schäfl,\u0026#32; B.,\u0026#32; Lehner,\u0026#32; J.,\u0026#32; Seidl,\u0026#32; P.,\u0026#32; Widrich,\u0026#32; M.,\u0026#32; Gruber,\u0026#32; L.,\u0026#32; Holzleitner,\u0026#32; M.,\u0026#32; Pavlovic,\u0026#32; M.,\u0026#32; Sandve,\u0026#32; G.,\u0026#32; Greiff,\u0026#32; V.,\u0026#32; Kreil,\u0026#32; D.,\u0026#32; Kopp,\u0026#32; M.,\u0026#32; Klambauer,\u0026#32; G.,\u0026#32; Brandstetter,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Hochreiter,\u0026#32; S. \u0026#32; (2020). \u0026#32;Hopfield networks is all you need. CoRR,\u0026#32;abs/2008.02217.\u0026#32;Retrieved from\u0026#32; https://arxiv.org/abs/2008.02217 [3] Janner \u0026#32;et al. Reinforcement learning as one big sequence modeling problem CoRR (2021) Janner,\u0026#32; M.,\u0026#32; Li,\u0026#32; Q.\u0026#32;\u0026amp;\u0026#32;Levine,\u0026#32; S. \u0026#32; (2021). \u0026#32;Reinforcement learning as one big sequence modeling problem. CoRR,\u0026#32;abs/2106.02039.\u0026#32;Retrieved from\u0026#32; https://arxiv.org/abs/2106.02039 [4] Gu \u0026#32;et al. Efficiently modeling long sequences with structured state spaces CoRR (2021) Gu,\u0026#32; A.,\u0026#32; Goel,\u0026#32; K.\u0026#32;\u0026amp;\u0026#32;Ré,\u0026#32; C. \u0026#32; (2021). \u0026#32;Efficiently modeling long sequences with structured state spaces. CoRR,\u0026#32;abs/2111.00396.\u0026#32;Retrieved from\u0026#32; https://arxiv.org/abs/2111.00396 [5] Chen \u0026#32;et al. Decision transformer: Reinforcement learning via sequence modeling (2021) Chen,\u0026#32; L.,\u0026#32; Lu,\u0026#32; K.,\u0026#32; Rajeswaran,\u0026#32; A.,\u0026#32; Lee,\u0026#32; K.,\u0026#32; Grover,\u0026#32; A.,\u0026#32; Laskin,\u0026#32; M.,\u0026#32; Abbeel,\u0026#32; P.,\u0026#32; Srinivas,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Mordatch,\u0026#32; I. \u0026#32; (2021). \u0026#32; Decision transformer: Reinforcement learning via sequence modeling. \u0026#32;Retrieved from\u0026#32; https://arxiv.org/abs/2106.01345 [6] Ha \u0026#32;\u0026amp;\u0026#32; Schmidhuber World models (2018) Ha,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Schmidhuber,\u0026#32; J. \u0026#32; (2018). \u0026#32;World models. https://doi.org/10.5281/ZENODO.1207631 [7] Weston \u0026#32;et al. Memory networks (2015) Weston,\u0026#32; J.,\u0026#32; Chopra,\u0026#32; S.\u0026#32;\u0026amp;\u0026#32;Bordes,\u0026#32; A. \u0026#32; (2015). \u0026#32; Memory networks. \u0026#32;Retrieved from\u0026#32; https://arxiv.org/abs/1410.3916 ","permalink":"https://shayanp.me/ideas/neural-memory-for-in-context-learning/","summary":"Can we combine features of both SSMs and Transformers to build a graph-like hierarchical memory mechanism with sublinear growth? Each node represents a concept, and edges connect concepts similar to an associative memory. The graph is traversed with attention mechanism to retrieve or add information.","title":"Dynamic Neural Memory for In-Context Learning: SSMs or Transformers?"},{"content":"","permalink":"https://shayanp.me/projects/probabilistic-homotopy-optimization-for-dynamic-motion-planning/","summary":"Inspired by Curriculum Learning and Probabilistic Roadmaps, PHO traverses a multidimensional homotopy space and discovers highly dynamic trajectories.","title":"Probabilistic Homotopy Optimization for Dynamic Motion Planning"},{"content":"","permalink":"https://shayanp.me/projects/novel-shape-generation-with-so3-equivariant-auto-encoders/","summary":"Designed an SO(3) equivariant autoencoder using spherical harmonics and a latent space traversal that separates rotation from deformation.","title":"Shape Generation with SO3-Equivariant Auto-Encoders"},{"content":"","permalink":"https://shayanp.me/projects/decision-ssm/","summary":"Reimplemented Decision Transformer replacing transformer with S4 model and demonstrated improved performance in credit assignment tasks.","title":"Decision State Space Model"},{"content":" ","permalink":"https://shayanp.me/projects/face-explore/","summary":"Created a face search engine that uses a custom clustering method on ResNet vector embeddings (unsupervised). The UI is a scalable website.","title":"Face Explore"},{"content":"","permalink":"https://shayanp.me/projects/formal-complexity-verification/","summary":"Formulated time complexity verification of a program as synthesizing a fix-point function. The demo uses a custom language with Python syntax.","title":"Formal Complexity Verification"},{"content":"","permalink":"https://shayanp.me/projects/scripty/","summary":"Educational tool to track student performance on projects, providing live feedback and tips, and automating infrastructure setup for instructors. Built with Python, DSPy, Kubernetes, and React; won Warp and Orbstack challenge prizes.","title":"Scripty"},{"content":"","permalink":"https://shayanp.me/projects/sharif-ai-challenge/","summary":"Developed AI agents for a multi-agent game, achieving 4th place in the competition. Implemented Huffman coding for cost-effective communication and utilized the A* algorithm for shortest pathfinding on a partially explored map.","title":"Sharif AI Challenge"}]